Experiments to try:
- how does adding deep layers affect each of our architectures differently?
- ex CNN larger context field or smth like that
- how much noise is the optimal amt to be added to train data?
- how useful is dropout, really? effects of it on diff architectures etc

- overfitting resNet
- maybe someone try lstm only (collapse the dims)
- (collapse to 1 channel) to do actual 2D conv

Paper writing:
- try removing each channel one at a time
- try training only on one subject
- signal processing stuff
