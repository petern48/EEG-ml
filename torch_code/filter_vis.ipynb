{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter Visualization in Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Mount Filesystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CYOGQe-Jo-ZW",
    "outputId": "e102bca8-01a6-4f9f-85f4-48e40571d00b"
   },
   "outputs": [],
   "source": [
    "GOOGLE = False\n",
    "if GOOGLE:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Check For Devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZGm3QtHho-ZX",
    "outputId": "ebbc906b-a95e-4b0c-a22e-04fbbcad9c37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please set GPU via Edit -> Notebook Settings.\n"
     ]
    }
   ],
   "source": [
    "# @title GPU code\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the device to use for training\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if torch.cuda.is_available():\n",
    "    print('Good to go!')\n",
    "else:\n",
    "    print('Please set GPU via Edit -> Notebook Settings.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def train_data_prep(X,y,sub_sample,average,noise, cutoff):\n",
    "\n",
    "    total_X = None\n",
    "    total_y = None\n",
    "\n",
    "    # Trimming the data (sample,22,1000) -> (sample,22,800)\n",
    "    X = X[:,:,0:cutoff]\n",
    "\n",
    "    # print('Shape of X after trimming:',X.shape)\n",
    "\n",
    "    # Maxpooling the data (sample,22,800) -> (sample,22,800/sub_sample)\n",
    "    X_max = np.max(X.reshape(X.shape[0], X.shape[1], -1, sub_sample), axis=3)\n",
    "\n",
    "\n",
    "    total_X = X_max\n",
    "    total_y = y\n",
    "    # print('Shape of X after maxpooling:',total_X.shape)\n",
    "\n",
    "    # Averaging + noise\n",
    "    X_average = np.mean(X.reshape(X.shape[0], X.shape[1], -1, average),axis=3)\n",
    "    X_average = X_average + np.random.normal(0.0, 0.5, X_average.shape)\n",
    "\n",
    "    total_X = np.vstack((total_X, X_average))\n",
    "    total_y = np.hstack((total_y, y))\n",
    "    # print('Shape of X after averaging+noise and concatenating:',total_X.shape)\n",
    "\n",
    "    # Subsampling\n",
    "\n",
    "    for i in range(sub_sample):\n",
    "\n",
    "        X_subsample = X[:, :, i::sub_sample] + \\\n",
    "                            (np.random.normal(0.0, 0.5, X[:, :,i::sub_sample].shape) if noise else 0.0)\n",
    "\n",
    "        total_X = np.vstack((total_X, X_subsample))\n",
    "        total_y = np.hstack((total_y, y))\n",
    "\n",
    "\n",
    "    print('Shape of X after subsampling and concatenating:',total_X.shape)\n",
    "    print('Shape of Y:',total_y.shape)\n",
    "    return total_X,total_y\n",
    "\n",
    "\n",
    "def test_data_prep(X, sub_sample, timestop):\n",
    "\n",
    "    total_X = None\n",
    "\n",
    "\n",
    "    # Trimming the data (sample,22,1000) -> (sample,22,800)\n",
    "    X = X[:,:,0:timestop]\n",
    "    # print('Shape of X after trimming:',X.shape)\n",
    "\n",
    "    # Maxpooling the data (sample,22,800) -> (sample,22,800/sub_sample)\n",
    "    X_max = np.max(X.reshape(X.shape[0], X.shape[1], -1, sub_sample), axis=3)\n",
    "\n",
    "\n",
    "    total_X = X_max\n",
    "    # print('Shape of X after maxpooling:',total_X.shape)\n",
    "\n",
    "    return total_X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluation Function Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Also, seed everything for reproducibility\n",
    "# code from https://gist.github.com/ihoromi4/b681a9088f348942b01711f251e5f964#file-seed_everything-py\n",
    "def seed_everything(seed: int):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "\n",
    "def train(model, train_loader, val_loader, optimizer, criterion, device,\n",
    "          num_epochs):\n",
    "    \"\"\"\n",
    "    Train the MLP classifier on the training set and evaluate it on the validation set every epoch.\n",
    "\n",
    "    Args:\n",
    "        model (MLP): MLP classifier to train.\n",
    "        train_loader (torch.utils.data.DataLoader): Data loader for the training set.\n",
    "        val_loader (torch.utils.data.DataLoader): Data loader for the validation set.\n",
    "        optimizertimestop (torch.optim.Optimizer): Optimizer to use for training.\n",
    "        criterion (callable): Loss function to use for training.\n",
    "        device (torch.device): Device to use for training.\n",
    "        num_epochs (int): Number of epochs to train the model.\n",
    "    \"\"\"\n",
    "    # Place model on device\n",
    "    model = model.to(device)\n",
    "    loss_history_train = []\n",
    "    loss_history_val = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "\n",
    "        # Use tqdm to display a progress bar during training\n",
    "        with tqdm(total=len(train_loader),\n",
    "                  desc=f'Epoch {epoch + 1}/{num_epochs}',\n",
    "                  position=0,\n",
    "                  leave=True) as pbar:\n",
    "            for inputs, labels in train_loader:\n",
    "                # Move inputs and labels to device\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # Zero out gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Compute the logits and loss\n",
    "                logits = model(inputs.float())\n",
    "                loss = criterion(logits, labels)\n",
    "\n",
    "                # Backpropagate the loss\n",
    "                loss.backward()\n",
    "\n",
    "                # Update the weights\n",
    "                optimizer.step()\n",
    "\n",
    "                # Update the progress bar\n",
    "                pbar.update(1)\n",
    "                pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "        # Evaluate the model on the validation set\n",
    "        avg_loss, accuracy = evaluate(model, val_loader, criterion, device)\n",
    "        loss_history_val.append(accuracy)\n",
    "        print(\n",
    "            f'Validation set: Average loss = {avg_loss:.4f}, Accuracy = {accuracy:.4f}'\n",
    "        )\n",
    "        avg_loss_train, accuracy_train = evaluate(model, train_loader, criterion, device)\n",
    "        print(\n",
    "            f'Training set: Average loss = {avg_loss_train:.4f}, Accuracy = {accuracy_train:.4f}'\n",
    "        )\n",
    "        loss_history_train.append(accuracy_train)\n",
    "    plt.plot(loss_history_val)\n",
    "    plt.plot(loss_history_train)\n",
    "    plt.show()\n",
    "    return loss_history_train, loss_history_val\n",
    "def evaluate(model, test_loader, criterion, device):\n",
    "    \"\"\"\n",
    "    Evaluate the MLP classifier on the test set.\n",
    "\n",
    "    Args:\n",
    "        model (MLP): MLP classifier to evaluate.\n",
    "        test_loader (torch.utils.data.DataLoader): Data loader for the test set.\n",
    "        criterion (callable): Loss function to use for evaluation.\n",
    "        device (torch.device): Device to use for evaluation.\n",
    "\n",
    "    Returns:\n",
    "        float: Average loss on the test set.\n",
    "        float: Accuracy on the test set.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0.0\n",
    "        num_correct = 0\n",
    "        num_samples = 0\n",
    "\n",
    "        for inputs, labels in test_loader:\n",
    "            # Move inputs and labels to device\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Compute the logits and loss\n",
    "            logits = model(inputs.float())\n",
    "            loss = criterion(logits, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Compute the accuracy\n",
    "            _, predictions = torch.max(logits, dim=1)\n",
    "            num_correct += (predictions == labels).sum().item()\n",
    "            num_samples += len(inputs)\n",
    "\n",
    "    # Compute the average loss and accuracy\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    accuracy = num_correct / num_samples\n",
    "\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## EEG Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "form",
    "id": "3bEjRyBso-ZX"
   },
   "outputs": [],
   "source": [
    "# @title EEG DATA\n",
    "import os\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "class EEG_Data(Dataset):\n",
    "\n",
    "    def __init__(self, root_dir, split, preprocess=lambda x,y:train_data_prep(x,y,2,2,True), transform=None, label_dict=None):\n",
    "        \"\"\"\n",
    "        Initialize the eeg dataset with the root directory for the images,\n",
    "        the split (train/val/test), an optional data transformation,\n",
    "        and an optional label dictionary.\n",
    "\n",
    "        Args:\n",
    "            root_dir (str): Root directory for the eeg images.\n",
    "            split (str): Split to use ('train', 'val', or 'test').\n",
    "            transform (callable, optional): Optional data transformation to apply to the images.\n",
    "            label_dict (dict, optional): Optional dictionary mapping integer labels to class names.\n",
    "        \"\"\"\n",
    "        assert split in ['train', 'val', 'test']\n",
    "        self.root_dir = root_dir\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        self.datastorch = []\n",
    "        self.labels = []\n",
    "        self.label_dict = [\"Cue Onset left\", \"Cue Onset right\", \"Cue onset foot\", \"Cue onset tongue\"]\n",
    "\n",
    "        ################# Your Implementations #################################\n",
    "        if self.split == 'train':\n",
    "            # First generating the training and validation indices using random splitting\n",
    "            X_train_valid = np.load(self.root_dir+\"X_train_valid.npy\")\n",
    "            y_train_valid = np.load(self.root_dir+\"y_train_valid.npy\")\n",
    "\n",
    "            np.random.seed(0)\n",
    "            data_length = len(X_train_valid)\n",
    "\n",
    "            ind_valid = np.random.choice(data_length, int(data_length*0.1), replace=False)\n",
    "            ind_train = np.array(list(set(range(data_length)).difference(set(ind_valid))))\n",
    "\n",
    "            # Creating the training and validation sets using the generated indices\n",
    "            (x_train, x_valid) = X_train_valid[ind_train], X_train_valid[ind_valid]\n",
    "            (y_train, y_valid) = y_train_valid[ind_train], y_train_valid[ind_valid]\n",
    "\n",
    "            if preprocess is not None:\n",
    "                x_train,y_train = preprocess(x_train,y_train)\n",
    "\n",
    "            self.datas = torch.from_numpy(x_train)\n",
    "            self.labels = [int(i-769) for i in torch.from_numpy(y_train)]\n",
    "\n",
    "        if self.split == 'val':\n",
    "            # First generating the training and validation indices using random splitting\n",
    "            X_train_valid = np.load(self.root_dir+\"X_train_valid.npy\")\n",
    "            y_train_valid = np.load(self.root_dir+\"y_train_valid.npy\")\n",
    "\n",
    "            data_length = len(X_train_valid)\n",
    "\n",
    "            np.random.seed(0)\n",
    "            ind_valid = np.random.choice(data_length, int(data_length*0.1), replace=False)\n",
    "            ind_train = np.array(list(set(range(data_length)).difference(set(ind_valid))))\n",
    "\n",
    "            # Creating the training and validation sets using the generated indices\n",
    "            (x_train, x_valid) = X_train_valid[ind_train], X_train_valid[ind_valid]\n",
    "            (y_train, y_valid) = y_train_valid[ind_train], y_train_valid[ind_valid]\n",
    "\n",
    "            if preprocess is not None:\n",
    "                x_valid,y_valid = preprocess(x_valid,y_valid)\n",
    "\n",
    "            self.datas = torch.from_numpy(x_valid)\n",
    "            self.labels = [int(i-769) for i in torch.from_numpy(y_valid)]\n",
    "\n",
    "        if self.split == 'test':\n",
    "            x_test = np.load(self.root_dir+\"X_test.npy\")\n",
    "            # x_test = test_data_prep(x_test_og)  # (2115, 1)  vals from 0-8 for participant\n",
    "            if preprocess is not None:\n",
    "                x_test = preprocess(x_test)\n",
    "            y_test = np.load(self.root_dir+\"y_test.npy\")  # (443, 1)\n",
    "            self.datas = torch.from_numpy(x_test)\n",
    "            self.labels = [int(i-769) for i in torch.from_numpy(y_test)]\n",
    "\n",
    "        ################# End of your Implementations ##########################\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the number of images in the dataset.\n",
    "\n",
    "        Returns:\n",
    "            int: Number of images in the dataset.\n",
    "        \"\"\"\n",
    "        dataset_len = 0\n",
    "        ################# Your Implementations #################################\n",
    "        # Return the number of images in the dataset\n",
    "        dataset_len = len(self.datas)\n",
    "        ################# End of your Implementations ##########################\n",
    "        return dataset_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        R10140    idx (int): Index of the image to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Tuple containing the image and its label.\n",
    "        \"\"\"\n",
    "        ################# Your Implementations #################################\n",
    "        # Load and preprocess image using self.root_dir,\n",
    "        # self.filenames[idx], and self.transform (if specified)\n",
    "\n",
    "        data = self.datas[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "        ################# End of your Implementations ##########################\n",
    "        return data, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EEGNet Torch Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "-VvgzKSCh_zs"
   },
   "outputs": [],
   "source": [
    "# This is EEGNet from https://arxiv.org/abs/1611.08024\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "class EEGNet(nn.Module):\n",
    "    def __init__(self, cutoff):\n",
    "        super(EEGNet, self).__init__()\n",
    "\n",
    "        self.F1 = 8\n",
    "        self.F2 = 16\n",
    "        self.D = 2\n",
    "\n",
    "        # Conv2d(in,out,kernel,stride,padding,bias)\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(1, self.F1, (1, 64), padding=(0, 32), bias=False),\n",
    "            nn.BatchNorm2d(self.F1)\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(self.F1, self.D*self.F1, (18, 1), groups=self.F1, bias=False),\n",
    "            nn.BatchNorm2d(self.D*self.F1),\n",
    "            nn.ELU(),\n",
    "            nn.AvgPool2d((1, 4)),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "\n",
    "        self.Conv3 = nn.Sequential(\n",
    "            nn.Conv2d(self.D*self.F1, self.D*self.F1, (1, 16), padding=(0, 8), groups=self.D*self.F1, bias=False),\n",
    "            nn.Conv2d(self.D*self.F1, self.F2, (1, 1), bias=False),\n",
    "            nn.BatchNorm2d(self.F2),\n",
    "            nn.ELU(),\n",
    "            nn.AvgPool2d((1, 8)),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "\n",
    "\n",
    "        if cutoff is not None:\n",
    "            cutoff_map ={\n",
    "                100 :80,\n",
    "                150:160,\n",
    "                200:240,\n",
    "                250:320,\n",
    "                300:320,\n",
    "                350:400,\n",
    "                400:480,\n",
    "                450:560,\n",
    "                500:560,\n",
    "                550:640,\n",
    "                600:720,\n",
    "                650:800,\n",
    "                700:880,\n",
    "                750:880,\n",
    "                800:960,\n",
    "                850:1040,\n",
    "                900:1120,\n",
    "                950:1200,\n",
    "                1000:1200\n",
    "            }\n",
    "            assert(cutoff in cutoff_map.keys())\n",
    "            self.flatten = nn.Flatten(start_dim=1)\n",
    "            self.classifier = nn.Linear(cutoff_map[cutoff], 4, bias=True)\n",
    "\n",
    "        else:\n",
    "            self.flatten = nn.Flatten(start_dim=1)\n",
    "            self.classifier = nn.Linear(240, 4, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.Conv3(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acc vs Time Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_result = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MafiXWN0EdYv",
    "outputId": "5eb43e6e-1b55-4655-92ca-146b294ff33e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X after subsampling and concatenating: (7616, 22, 200)\n",
      "Shape of Y: (7616,)\n",
      "Shape of X after subsampling and concatenating: (844, 22, 200)\n",
      "Shape of Y: (844,)\n",
      "val split:  844\n",
      "train split:  7616\n",
      "torch.Size([1, 22, 200])\n",
      "torch.Size([1, 18, 200])\n",
      "avg_loss 0.7750040718487331\n",
      "accuracy 0.7110609480812641\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "# @title Data loading\n",
    "cutoff = 400\n",
    "data_root = \"../project_data/project/\"\n",
    "if GOOGLE:\n",
    "    data_root = \"/contentimestopt/drive/MyDrive/project/\"\n",
    "data_transform =  lambda x: (x.reshape(1, x.shape[0],x.shape[1]))\n",
    "preprocess = lambda x,y:train_data_prep(x,y,2,2,True, cutoff)\n",
    "# Create eeg dataset object\n",
    "eeg_train = EEG_Data(data_root,\n",
    "                            split='train',\n",
    "                            preprocess=preprocess,\n",
    "                            transform=data_transform)\n",
    "\n",
    "eeg_val = EEG_Data(data_root,\n",
    "                        split='val',\n",
    "                        preprocess=preprocess,\n",
    "                        transform=data_transform)\n",
    "eeg_test = EEG_Data(data_root,\n",
    "                        split='test',\n",
    "                        preprocess=lambda x:test_data_prep(x, 2, cutoff),\n",
    "                        transform=data_transform)\n",
    "print(\"val split: \", len(eeg_val))\n",
    "print(\"train split: \", len(eeg_train))\n",
    "print(eeg_train[0][0].shape)\n",
    "print(eeg_train[0][0][:,0:18,:].shape)\n",
    "# Create the dataloaders\n",
    "# Define the batch size and number of workers\n",
    "batch_size = 64\n",
    "num_workers=2\n",
    "# Create DataLoader for trainimport torch\n",
    "train_loader = DataLoader(eeg_train,\n",
    "                        batch_size=batch_size,\n",
    "                        num_workers=num_workers,\n",
    "                        shuffle=True)\n",
    "val_loader = DataLoader(eeg_val,\n",
    "                        batch_size=batch_size,\n",
    "                        num_workers=num_workers,\n",
    "                        shuffle=False)\n",
    "test_loader = DataLoader(eeg_test,\n",
    "                        batch_size=batch_size,\n",
    "                        num_workers=num_workers,\n",
    "                        shuffle=False)\n",
    "\n",
    "seed_everything(0)\n",
    "\n",
    "\n",
    "model = EEGNet(cutoff)\n",
    "model.load_state_dict(torch.load(\"./\"+str(cutoff)+\".zip\"))\n",
    "\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Let's test_loaderss function, your implementation and the built-in loss function should\n",
    "# be almost identical.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "avg_loss, accuracy = evaluate(model, test_loader, criterion, device)\n",
    "\n",
    "json.dump(all_result, open(\"./results.json\", \"w\"))\n",
    "print(\"avg_loss\", avg_loss)\n",
    "print(\"accuracy\", accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import utils\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "\n",
    "# self.F1 = 8\n",
    "# self.F2 = 16\n",
    "# self.D = 2\n",
    "\n",
    "# # Conv2d(in,out,kernel,stride,padding,bias)\n",
    "# self.conv1 = nn.Sequential(\n",
    "#     nn.Conv2d(1, self.F1, (1, 64), padding=(0, 32), bias=False),\n",
    "#     nn.BatchNorm2d(self.F1)\n",
    "# )\n",
    "\n",
    "# self.conv2 = nn.Sequential(\n",
    "#     nn.Conv2d(self.F1, self.D*self.F1, (18, 1), groups=self.F1, bias=False),\n",
    "#     nn.BatchNorm2d(self.D*self.F1),\n",
    "#     nn.ELU(),\n",
    "#     nn.AvgPool2d((1, 4)),\n",
    "#     nn.Dropout(0.5)\n",
    "# )\n",
    "\n",
    "# self.Conv3 = nn.Sequential(\n",
    "#     nn.Conv2d(self.D*self.F1, self.D*self.F1, (1, 16), padding=(0, 8), groups=self.D*self.F1, bias=False),\n",
    "#     nn.Conv2d(self.D*self.F1, self.F2, (1, 1), bias=False),\n",
    "#     nn.BatchNorm2d(self.F2),\n",
    "#     nn.ELU(),\n",
    "#     nn.AvgPool2d((1, 8)),\n",
    "#     nn.Dropout(0.5)\n",
    "# )\n",
    "\n",
    "figure(figsize=(8, 6), dpi=80)\n",
    "def visTensor(tensor, ch=0, allkernels=False, nrow=1, padding=1): \n",
    "    n,c,w,h = tensor.shape\n",
    "\n",
    "    if allkernels: tensor = tensor.view(n*c, -1, w, h)\n",
    "    elif c != 3: tensor = tensor[:,ch,:,:].unsqueeze(dim=1)\n",
    "\n",
    "    rows = np.min((tensor.shape[0] // nrow + 1, 64))    \n",
    "    grid = utils.make_grid(tensor, nrow=nrow, normalize=True, padding=padding)\n",
    "    # plt.figure( figsize=(nrow,rows) )\n",
    "    plt.imshow(grid.numpy().transpose((1, 2, 0)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "filter = model.conv1[0].weight.data.clone()\n",
    "visTensor(filter, ch=0, allkernels=False)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qp31VVTp2pgf",
    "outputId": "da86b7db-82f0-4c37-bb9c-56c5717e81dd"
   },
   "outputs": [],
   "source": [
    "\n",
    "avg_loss, accuracy = evaluate(model, test_loader, criterion, device)\n",
    "\n",
    "print(\"avg_loss\", avg_loss)\n",
    "print(\"accuracy\", accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
