{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOGLE = False\n",
    "if GOOGLE:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shared.data_loader import *\n",
    "from shared.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please set GPU via Edit -> Notebook Settings.\n"
     ]
    }
   ],
   "source": [
    "# GPU code\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the device to use for training\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if torch.cuda.is_available():\n",
    "    print('Good to go!')\n",
    "else:\n",
    "    print('Please set GPU via Edit -> Notebook Settings.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform code\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "\n",
    "def scale_high_freq(input_signal, cutoff_frequency, factor):\n",
    "    # Design a high-pass filter\n",
    "    b, a = signal.butter(4, cutoff_frequency, 'high')\n",
    "\n",
    "    # Apply the high-pass filter\n",
    "    highpass_filtered_signal = signal.filtfilt(b, a, input_signal)\n",
    "\n",
    "    # Scale the amplitude of the high-pass filtered signal\n",
    "    scaled_highpass_signal = highpass_filtered_signal * factor\n",
    "\n",
    "    # Combine the scaled high-pass signal with the original signal\n",
    "    output_signal = input_signal - highpass_filtered_signal + scaled_highpass_signal\n",
    "\n",
    "    return output_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EEG DATA\n",
    "import os\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "class EEG_Data(Dataset):\n",
    "\n",
    "    def __init__(self, root_dir, split, preprocess=lambda x,y:train_data_prep(x,y,2,2,True), transform=None, label_dict=None):\n",
    "        \"\"\"\n",
    "        Initialize the eeg dataset with the root directory for the images,\n",
    "        the split (train/val/test), an optional data transformation,\n",
    "        and an optional label dictionary.\n",
    "\n",
    "        Args:\n",
    "            root_dir (str): Root directory for the eeg images.\n",
    "            split (str): Split to use ('train', 'val', or 'test').\n",
    "            transform (callable, optional): Optional data transformation to apply to the images.\n",
    "            label_dict (dict, optional): Optional dictionary mapping integer labels to class names.\n",
    "        \"\"\"\n",
    "        assert split in ['train', 'val', 'test']\n",
    "        self.root_dir = root_dir\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        self.datastorch = []\n",
    "        self.labels = []\n",
    "        self.label_dict = [\"Cue Onset left\", \"Cue Onset right\", \"Cue onset foot\", \"Cue onset tongue\"]\n",
    "\n",
    "        ################# Your Implementations #################################\n",
    "        if self.split == 'train':\n",
    "            # First generating the training and validation indices using random splitting\n",
    "            X_train_valid = np.load(self.root_dir+\"X_train_valid.npy\")\n",
    "            y_train_valid = np.load(self.root_dir+\"y_train_valid.npy\")\n",
    "\n",
    "            np.random.seed(0)\n",
    "            data_length = len(X_train_valid)\n",
    "\n",
    "            ind_valid = np.random.choice(data_length, int(data_length*0.2), replace=False)\n",
    "            ind_train = np.array(list(set(range(data_length)).difference(set(ind_valid))))\n",
    "\n",
    "            # Creating the training and validation sets using the generated indices\n",
    "            (x_train, x_valid) = X_train_valid[ind_train], X_train_valid[ind_valid]\n",
    "            (y_train, y_valid) = y_train_valid[ind_train], y_train_valid[ind_valid]\n",
    "\n",
    "            if preprocess is not None:\n",
    "                x_train,y_train = preprocess(x_train,y_train)\n",
    "\n",
    "            self.datas = torch.from_numpy(x_train)\n",
    "            self.labels = [int(i-769) for i in torch.from_numpy(y_train)]\n",
    "\n",
    "        if self.split == 'val':\n",
    "            # First generating the training and validation indices using random splitting\n",
    "            X_train_valid = np.load(self.root_dir+\"X_train_valid.npy\")\n",
    "            y_train_valid = np.load(self.root_dir+\"y_train_valid.npy\")\n",
    "\n",
    "            np.random.seed(0)\n",
    "            data_length = len(X_train_valid)\n",
    "\n",
    "            ind_valid = np.random.choice(data_length, int(data_length*0.2), replace=False)\n",
    "            ind_train = np.array(list(set(range(data_length)).difference(set(ind_valid))))\n",
    "\n",
    "            # Creating the training and validation sets using the generated indices\n",
    "            (x_train, x_valid) = X_train_valid[ind_train], X_train_valid[ind_valid]\n",
    "            (y_train, y_valid) = y_train_valid[ind_train], y_train_valid[ind_valid]\n",
    "\n",
    "            if preprocess is not None:\n",
    "                x_valid,y_valid = preprocess(x_valid,y_valid)\n",
    "\n",
    "            self.datas = torch.from_numpy(x_valid)\n",
    "            self.labels = [int(i-769) for i in torch.from_numpy(y_valid)]\n",
    "\n",
    "        if self.split == 'test':\n",
    "            x_test_og = np.load(self.root_dir+\"X_test.npy\")\n",
    "            # x_test = test_data_prep(x_test_og)  # (2115, 1)  vals from 0-8 for participant\n",
    "            y_test = np.load(self.root_dir+\"y_test.npy\")  # (443, 1)\n",
    "            self.datas = torch.from_numpy(x_test_og)\n",
    "            self.labels = [int(i-769) for i in torch.from_numpy(y_test)]\n",
    "\n",
    "        ################# End of your Implementations ##########################\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the number of images in the dataset.\n",
    "\n",
    "        Returns:\n",
    "            int: Number of images in the dataset.\n",
    "        \"\"\"\n",
    "        dataset_len = 0\n",
    "        ################# Your Implementations #################################\n",
    "        # Return the number of images in the dataset\n",
    "        dataset_len = len(self.datas)\n",
    "        ################# End of your Implementations ##########################\n",
    "        return dataset_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        R10140    idx (int): Index of the image to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Tuple containing the image and its label.\n",
    "        \"\"\"\n",
    "        ################# Your Implementations #################################\n",
    "        # Load and preprocess image using self.root_dir,\n",
    "        # self.filenames[idx], and self.transform (if specified)\n",
    "\n",
    "        data = self.datas[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "        ################# End of your Implementations ##########################\n",
    "        return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2d Convolution\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "fc_block = lambda i, o : nn.Linear(i,o)\n",
    "\n",
    "\n",
    "class ConvBlock2D(nn.Module):\n",
    "    def __init__(self, input_size, output_size, kernel_size, dropout):\n",
    "        super().__init__()\n",
    "        try:\n",
    "          padding = (kernel_size-1)//2\n",
    "        except:\n",
    "          padding = ((kernel_size[0]-1)//2, (kernel_size[1]-1)//2)\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=input_size, out_channels=output_size, kernel_size=kernel_size, padding=padding),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(output_size),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x, **kwargs):\n",
    "        # keey the residual connection here\n",
    "        return self.conv_block.forward(x)\n",
    "\n",
    "\n",
    "class ResBlock2D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size = 3, dropout=0.6,  downsample=False):\n",
    "        '''\n",
    "        Initialize a basic residual block.\n",
    "        Depending on whether downsample is True, there are two types of basic blocks in Resnet 18,\n",
    "        one to downsample the input and the other maintains the same size.\n",
    "        You can use Pytorch's functions.\n",
    "\n",
    "        Input and output shapes of each layer:\n",
    "        1) conv1 (3*3 kernel, no bias): (batch_size, in_channels, H, W) -> (batch_size, out_channels, H, W) if downsample=False\n",
    "           conv1 (3*3 kernel, no bias): (batch_size, in_channels, H, W) -> (batch_size, out_channels, H//2, W//2) if downsample=True, with stride of 2\n",
    "        2) conv2 (3*3 kernel, no bias): (batch_size, out_channels, H, W) -> (batch_size, out_channels, H, W)\n",
    "        3) conv3 (optional) if downsample=True (1*1 kernel, no bias): (batch_size, in_channels, H, W) -> (batch_size, out_channels, H//2, W//2) with stride of 2\n",
    "        '''\n",
    "        super().__init__()\n",
    "        ################# Your Implementations #################################\n",
    "        self.downsample = downsample\n",
    "        if (in_channels != out_channels):\n",
    "           self.downsample = True\n",
    "        try:\n",
    "          padding = (kernel_size-1)//2\n",
    "        except:\n",
    "          padding = ((kernel_size[0]-1)//2, (kernel_size[1]-1)//2)\n",
    "        self.kernel_size = kernel_size\n",
    "        self.dropout = dropout\n",
    "        # self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, bias=False, padding=1) #(128 - 3 + 2*1)//2 + 1\n",
    "        self.conv2 = nn.Sequential(\n",
    "                        nn.Conv2d(out_channels, out_channels, kernel_size = kernel_size, stride = 1, padding = padding, bias=False),\n",
    "                        nn.BatchNorm2d(out_channels))\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.bn3 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.relu2 = nn.ReLU()\n",
    "        if self.downsample:\n",
    "          self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=1, padding=1, bias=False) #(128 - 3 + 2*1)//2 + 1\n",
    "          self.conv3 = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False) #(128 - 3 + 2*1)//2 + 1\n",
    "        else:\n",
    "          self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False) #(128 - 3 + 2*1)//2 + 1\n",
    "\n",
    "        ################# End of your Implementations ##########################\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the basic residual block.\n",
    "        The input tensor 'x' should pass through the following layers:\n",
    "        1) conv1: (batch_size, in_channels, H, W) -> (batch_size, out_channels, H, W)\n",
    "           conv1: (batch_size, in_channels, H, W) -> (batch_size, out_channels, H//2, W//2) if downsample=True\n",
    "        2) Apply batch normalization after conv1.\n",
    "        3) Apply relu activation.\n",
    "        4) conv2: (batch_size, out_channels, H, W) -> (batch_size, out_channels, H, W)\n",
    "        5) Apply batch normalization after conv2.\n",
    "        6) (Optional) if downsample=True, conv3: (batch_size, in_channels, H, W) -> (batch_size, out_channels, H//2, W//2) on the original input\n",
    "        7) (Optional) if downsample=True, apply batch normalization after conv3.\n",
    "        8) Add the residual value to the original input\n",
    "        9) Apply relu activation in the end.\n",
    "        \"\"\"\n",
    "        ################# Your Implementations #################################\n",
    "        # TODO: Implement the forward pass of the basic residual block.\n",
    "        og=x\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        if self.downsample:\n",
    "          og = self.conv3(og)\n",
    "          og = self.bn3(og)\n",
    "        x += og\n",
    "        x =self.relu2(x)\n",
    "        x = nn.Dropout(self.dropout)(x)\n",
    "        ################# End of your Implementations ##########################\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GOOGLE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Data loading\u001b[39;00m\n\u001b[1;32m      2\u001b[0m data_root \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../project_data/project/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mGOOGLE\u001b[49m:\n\u001b[1;32m      4\u001b[0m     data_root \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/content/drive/MyDrive/project/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m data_transform \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mabs\u001b[39m(np\u001b[38;5;241m.\u001b[39mfft\u001b[38;5;241m.\u001b[39mfft(x))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'GOOGLE' is not defined"
     ]
    }
   ],
   "source": [
    "# Data loading\n",
    "data_root = \"../project_data/project/\"\n",
    "if GOOGLE:\n",
    "    data_root = \"/content/drive/MyDrive/project/\"\n",
    "data_transform = lambda x: abs(np.fft.fft(x))\n",
    "data_transform = lambda x : scale_high_freq(x, 0.06, 0.4)\n",
    "data_transform =  lambda x: x.reshape(1,x.shape[0],x.shape[1]) # fft transformation\n",
    "\n",
    "# Create eeg dataset object\n",
    "eeg_train = EEG_Data(data_root,\n",
    "                              split='train',\n",
    "                              preprocess=lambda x,y:(x,y),\n",
    "                              transform=data_transform)\n",
    "\n",
    "eeg_val = EEG_Data(data_root,\n",
    "                            split='val',\n",
    "                            preprocess=lambda x,y:(x,y),\n",
    "                            transform=data_transform)\n",
    "eeg_test = EEG_Data(data_root,\n",
    "                            split='test',\n",
    "                            preprocess=lambda x,y:(x,0),\n",
    "                            transform=None)\n",
    "# Create the dataloaders\n",
    "# Define the batch size and number of workers\n",
    "batch_size = 64\n",
    "num_workers=2\n",
    "# Create DataLoader for training and validation sets\n",
    "train_loader = DataLoader(eeg_train,\n",
    "                          batch_size=batch_size,\n",
    "                          num_workers=num_workers,\n",
    "                        shuffle=True)\n",
    "val_loader = DataLoader(eeg_val,\n",
    "                        batch_size=batch_size,\n",
    "                        num_workers=num_workers,\n",
    "                        shuffle=False)\n",
    "test_loader = DataLoader(eeg_test,\n",
    "                        batch_size=batch_size,\n",
    "                        num_workers=num_workers,\n",
    "                        shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnetv2 =  nn.Sequential(\n",
    "    ConvBlock2D(1,50,(22,70), 0.5),\n",
    "    ConvBlock2D(50,100,(22,70), 0.5),\n",
    "    ConvBlock2D(100,120,(22,70), 0.5),\n",
    "    nn.Flatten(start_dim=1),\n",
    "    fc_block(1800, 40),\n",
    "    nn.BatchNorm1d(40),\n",
    "    nn.ReLU(),\n",
    "    fc_block(40,4)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/30:   0%|          | 0/27 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "\n",
    "seed_everything(0)\n",
    "# train_laoder.160transform =  lambda x: x.reshape(1,x.shape[0],x.shape[1]) # fft transformation\n",
    "train_loader.transform = None # fft transformation\n",
    "\n",
    "\n",
    "model = resnetv2\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# Let's use the built-in optimizer for a full version of SGD optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# For loss function, your implementation and the built-in loss function should\n",
    "# be almost identical.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train the model\n",
    "train(model,\n",
    "      train_loader,\n",
    "      test_loader,\n",
    "      optimizer,\n",
    "      criterion,\n",
    "      device,\n",
    "      num_epochs=30)\n",
    "\n",
    "avg_loss, accuracy = evaluate(model, test_loader, criterion, device)\n",
    "\n",
    "print(\"avg_loss\", avg_loss)\n",
    "print(\"accuracy\", accuracy)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
