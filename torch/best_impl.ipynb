{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter Visualization in Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Mount Filesystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CYOGQe-Jo-ZW",
    "outputId": "e102bca8-01a6-4f9f-85f4-48e40571d00b"
   },
   "outputs": [],
   "source": [
    "GOOGLE = False\n",
    "if GOOGLE:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Check For Devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZGm3QtHho-ZX",
    "outputId": "ebbc906b-a95e-4b0c-a22e-04fbbcad9c37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please set GPU via Edit -> Notebook Settings.\n"
     ]
    }
   ],
   "source": [
    "# @title GPU code\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the device to use for training\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if torch.cuda.is_available():\n",
    "    print('Good to go!')\n",
    "else:\n",
    "    print('Please set GPU via Edit -> Notebook Settings.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def train_data_prep(X,y,sub_sample,average,noise, cutoff):\n",
    "\n",
    "    total_X = None\n",
    "    total_y = None\n",
    "\n",
    "    # Trimming the data (sample,22,1000) -> (sample,22,800)\n",
    "    X = X[:,:,0:cutoff]\n",
    "\n",
    "    # print('Shape of X after trimming:',X.shape)\n",
    "\n",
    "    # Maxpooling the data (sample,22,800) -> (sample,22,800/sub_sample)\n",
    "    X_max = np.max(X.reshape(X.shape[0], X.shape[1], -1, sub_sample), axis=3)\n",
    "\n",
    "\n",
    "    total_X = X_max\n",
    "    total_y = y\n",
    "    # print('Shape of X after maxpooling:',total_X.shape)\n",
    "\n",
    "    # Averaging + noise\n",
    "    X_average = np.mean(X.reshape(X.shape[0], X.shape[1], -1, average),axis=3)\n",
    "    X_average = X_average + np.random.normal(0.0, 1, X_average.shape)\n",
    "\n",
    "    total_X = np.vstack((total_X, X_average))\n",
    "    total_y = np.hstack((total_y, y))\n",
    "    # print('Shape of X after averaging+noise and concatenating:',total_X.shape)\n",
    "\n",
    "    # Subsampling\n",
    "\n",
    "    for i in range(sub_sample):\n",
    "\n",
    "        X_subsample = X[:, :, i::sub_sample] + \\\n",
    "                            (np.random.normal(0.0, 0.5, X[:, :,i::sub_sample].shape) if noise else 0.0)\n",
    "\n",
    "        total_X = np.vstack((total_X, X_subsample))\n",
    "        total_y = np.hstack((total_y, y))\n",
    "\n",
    "\n",
    "    print('Shape of X after subsampling and concatenating:',total_X.shape)\n",
    "    print('Shape of Y:',total_y.shape)\n",
    "    return total_X,total_y\n",
    "\n",
    "\n",
    "def test_data_prep(X, sub_sample, timestop):\n",
    "\n",
    "    total_X = None\n",
    "\n",
    "\n",
    "    # Trimming the data (sample,22,1000) -> (sample,22,800)\n",
    "    X = X[:,:,0:timestop]\n",
    "    # print('Shape of X after trimming:',X.shape)\n",
    "\n",
    "    # Maxpooling the data (sample,22,800) -> (sample,22,800/sub_sample)\n",
    "    X_max = np.max(X.reshape(X.shape[0], X.shape[1], -1, sub_sample), axis=3)\n",
    "\n",
    "\n",
    "    total_X = X_max\n",
    "    # print('Shape of X after maxpooling:',total_X.shape)\n",
    "\n",
    "    return total_X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluation Function Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Also, seed everything for reproducibility\n",
    "# code from https://gist.github.com/ihoromi4/b681a9088f348942b01711f251e5f964#file-seed_everything-py\n",
    "def seed_everything(seed: int):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "\n",
    "def train(model, train_loader, val_loader, optimizer, criterion, device,\n",
    "          num_epochs):\n",
    "    \"\"\"\n",
    "    Train the MLP classifier on the training set and evaluate it on the validation set every epoch.\n",
    "\n",
    "    Args:\n",
    "        model (MLP): MLP classifier to train.\n",
    "        train_loader (torch.utils.data.DataLoader): Data loader for the training set.\n",
    "        val_loader (torch.utils.data.DataLoader): Data loader for the validation set.\n",
    "        optimizertimestop (torch.optim.Optimizer): Optimizer to use for training.\n",
    "        criterion (callable): Loss function to use for training.\n",
    "        device (torch.device): Device to use for training.\n",
    "        num_epochs (int): Number of epochs to train the model.\n",
    "    \"\"\"\n",
    "    # Place model on device\n",
    "    model = model.to(device)\n",
    "    loss_history_train = []\n",
    "    loss_history_val = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "\n",
    "        # Use tqdm to display a progress bar during training\n",
    "        with tqdm(total=len(train_loader),\n",
    "                  desc=f'Epoch {epoch + 1}/{num_epochs}',\n",
    "                  position=0,\n",
    "                  leave=True) as pbar:\n",
    "            for inputs, labels in train_loader:\n",
    "                # Move inputs and labels to device\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # Zero out gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Compute the logits and loss\n",
    "                logits = model(inputs.float())\n",
    "                loss = criterion(logits, labels)\n",
    "\n",
    "                # Backpropagate the loss\n",
    "                loss.backward()\n",
    "\n",
    "                # Update the weights\n",
    "                optimizer.step()\n",
    "\n",
    "                # Update the progress bar\n",
    "                pbar.update(1)\n",
    "                pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "        # Evaluate the model on the validation set\n",
    "        avg_loss, accuracy = evaluate(model, val_loader, criterion, device)\n",
    "        loss_history_val.append(accuracy)\n",
    "        print(\n",
    "            f'Validation set: Average loss = {avg_loss:.4f}, Accuracy = {accuracy:.4f}'\n",
    "        )\n",
    "        avg_loss_train, accuracy_train = evaluate(model, train_loader, criterion, device)\n",
    "        print(\n",
    "            f'Training set: Average loss = {avg_loss_train:.4f}, Accuracy = {accuracy_train:.4f}'\n",
    "        )\n",
    "        loss_history_train.append(accuracy_train)\n",
    "    plt.plot(loss_history_val)\n",
    "    plt.plot(loss_history_train)\n",
    "    plt.show()\n",
    "    return loss_history_train, loss_history_val\n",
    "def evaluate(model, test_loader, criterion, device):\n",
    "    \"\"\"\n",
    "    Evaluate the MLP classifier on the test set.\n",
    "\n",
    "    Args:\n",
    "        model (MLP): MLP classifier to evaluate.\n",
    "        test_loader (torch.utils.data.DataLoader): Data loader for the test set.\n",
    "        criterion (callable): Loss function to use for evaluation.\n",
    "        device (torch.device): Device to use for evaluation.\n",
    "\n",
    "    Returns:\n",
    "        float: Average loss on the test set.\n",
    "        float: Accuracy on the test set.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0.0\n",
    "        num_correct = 0\n",
    "        num_samples = 0\n",
    "\n",
    "        for inputs, labels in test_loader:\n",
    "            # Move inputs and labels to device\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Compute the logits and loss\n",
    "            logits = model(inputs.float())\n",
    "            loss = criterion(logits, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Compute the accuracy\n",
    "            _, predictions = torch.max(logits, dim=1)\n",
    "            num_correct += (predictions == labels).sum().item()\n",
    "            num_samples += len(inputs)\n",
    "\n",
    "    # Compute the average loss and accuracy\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    accuracy = num_correct / num_samples\n",
    "\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## EEG Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "form",
    "id": "3bEjRyBso-ZX"
   },
   "outputs": [],
   "source": [
    "# @title EEG DATA\n",
    "import os\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "class EEG_Data(Dataset):\n",
    "\n",
    "    def __init__(self, root_dir, split, preprocess=lambda x,y:train_data_prep(x,y,2,2,True), transform=None, label_dict=None):\n",
    "        \"\"\"\n",
    "        Initialize the eeg dataset with the root directory for the images,\n",
    "        the split (train/val/test), an optional data transformation,\n",
    "        and an optional label dictionary.\n",
    "\n",
    "        Args:\n",
    "            root_dir (str): Root directory for the eeg images.\n",
    "            split (str): Split to use ('train', 'val', or 'test').\n",
    "            transform (callable, optional): Optional data transformation to apply to the images.\n",
    "            label_dict (dict, optional): Optional dictionary mapping integer labels to class names.\n",
    "        \"\"\"\n",
    "        assert split in ['train', 'val', 'test']\n",
    "        self.root_dir = root_dir\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        self.datastorch = []\n",
    "        self.labels = []\n",
    "        self.label_dict = [\"Cue Onset left\", \"Cue Onset right\", \"Cue onset foot\", \"Cue onset tongue\"]\n",
    "\n",
    "        ################# Your Implementations #################################\n",
    "        if self.split == 'train':\n",
    "            # First generating the training and validation indices using random splitting\n",
    "            X_train_valid = np.load(self.root_dir+\"X_train_valid.npy\")\n",
    "            y_train_valid = np.load(self.root_dir+\"y_train_valid.npy\")\n",
    "\n",
    "            np.random.seed(0)\n",
    "            data_length = len(X_train_valid)\n",
    "\n",
    "            ind_valid = np.random.choice(data_length, int(data_length*0.1), replace=False)\n",
    "            ind_train = np.array(list(set(range(data_length)).difference(set(ind_valid))))\n",
    "\n",
    "            # Creating the training and validation sets using the generated indices\n",
    "            (x_train, x_valid) = X_train_valid[ind_train], X_train_valid[ind_valid]\n",
    "            (y_train, y_valid) = y_train_valid[ind_train], y_train_valid[ind_valid]\n",
    "\n",
    "            if preprocess is not None:\n",
    "                x_train,y_train = preprocess(x_train,y_train)\n",
    "\n",
    "            self.datas = torch.from_numpy(x_train)\n",
    "            self.labels = [int(i-769) for i in torch.from_numpy(y_train)]\n",
    "\n",
    "        if self.split == 'val':\n",
    "            # First generating the training and validation indices using random splitting\n",
    "            X_train_valid = np.load(self.root_dir+\"X_train_valid.npy\")\n",
    "            y_train_valid = np.load(self.root_dir+\"y_train_valid.npy\")\n",
    "\n",
    "            data_length = len(X_train_valid)\n",
    "\n",
    "            np.random.seed(0)\n",
    "            ind_valid = np.random.choice(data_length, int(data_length*0.1), replace=False)\n",
    "            ind_train = np.array(list(set(range(data_length)).difference(set(ind_valid))))\n",
    "\n",
    "            # Creating the training and validation sets using the generated indices\n",
    "            (x_train, x_valid) = X_train_valid[ind_train], X_train_valid[ind_valid]\n",
    "            (y_train, y_valid) = y_train_valid[ind_train], y_train_valid[ind_valid]\n",
    "\n",
    "            if preprocess is not None:\n",
    "                x_valid,y_valid = preprocess(x_valid,y_valid)\n",
    "\n",
    "            self.datas = torch.from_numpy(x_valid)\n",
    "            self.labels = [int(i-769) for i in torch.from_numpy(y_valid)]\n",
    "\n",
    "        if self.split == 'test':\n",
    "            x_test = np.load(self.root_dir+\"X_test.npy\")\n",
    "            # x_test = test_data_prep(x_test_og)  # (2115, 1)  vals from 0-8 for participant\n",
    "            if preprocess is not None:\n",
    "                x_test = preprocess(x_test)\n",
    "            y_test = np.load(self.root_dir+\"y_test.npy\")  # (443, 1)\n",
    "            self.datas = torch.from_numpy(x_test)\n",
    "            self.labels = [int(i-769) for i in torch.from_numpy(y_test)]\n",
    "\n",
    "        ################# End of your Implementations ##########################\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the number of images in the dataset.\n",
    "\n",
    "        Returns:\n",
    "            int: Number of images in the dataset.\n",
    "        \"\"\"\n",
    "        dataset_len = 0\n",
    "        ################# Your Implementations #################################\n",
    "        # Return the number of images in the dataset\n",
    "        dataset_len = len(self.datas)\n",
    "        ################# End of your Implementations ##########################\n",
    "        return dataset_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        R10140    idx (int): Index of the image to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Tuple containing the image and its label.\n",
    "        \"\"\"\n",
    "        ################# Your Implementations #################################\n",
    "        # Load and preprocess image using self.root_dir,\n",
    "        # self.filenames[idx], and self.transform (if specified)\n",
    "\n",
    "        data = self.datas[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "        ################# End of your Implementations ##########################\n",
    "        return data, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EEGNet Torch Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "-VvgzKSCh_zs"
   },
   "outputs": [],
   "source": [
    "# This is EEGNet from https://arxiv.org/abs/1611.08024\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "class EEGNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EEGNet, self).__init__()\n",
    "\n",
    "        self.F1 = 8\n",
    "        self.F2 = 16\n",
    "        self.D = 2\n",
    "\n",
    "        # Conv2d(in,out,kernel,stride,padding,bias)\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(1, self.F1, (1, 64), padding=(0, 32), bias=False),\n",
    "            nn.BatchNorm2d(self.F1)\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(self.F1, self.D*self.F1, (18, 1), groups=self.F1, bias=False),\n",
    "            nn.BatchNorm2d(self.D*self.F1),\n",
    "            nn.ELU(),\n",
    "            nn.AvgPool2d((1, 4)),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "\n",
    "        self.Conv3 = nn.Sequential(\n",
    "            nn.Conv2d(self.D*self.F1, self.D*self.F1, (1, 16), padding=(0, 8), groups=self.D*self.F1, bias=False),\n",
    "            nn.Conv2d(self.D*self.F1, self.F2, (1, 1), bias=False),\n",
    "            nn.BatchNorm2d(self.F2),\n",
    "            nn.ELU(),\n",
    "            nn.AvgPool2d((1, 8)),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "\n",
    "        self.flatten = nn.Flatten(start_dim=1)\n",
    "        self.classifier = nn.Linear(240, 4, bias=True)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.Conv3(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X after subsampling and concatenating: (7616, 22, 500)\n",
      "Shape of Y: (7616,)\n",
      "Shape of X after subsampling and concatenating: (844, 22, 500)\n",
      "Shape of Y: (844,)\n",
      "val split:  844\n",
      "train split:  7616\n",
      "torch.Size([1, 18, 500])\n",
      "torch.Size([1, 18, 500])\n"
     ]
    }
   ],
   "source": [
    "cutoff = 1000\n",
    "data_root = \"../project_data/project/\"\n",
    "if GOOGLE:\n",
    "    data_root = \"/contentimestopt/drive/MyDrive/project/\"\n",
    "data_transform =  lambda x: (x.reshape(1, x.shape[0],x.shape[1]))\n",
    "preprocess = lambda x,y:train_data_prep(x,y,2,2,True, cutoff)\n",
    "# Create eeg dataset object\n",
    "eeg_train = EEG_Data(data_root,\n",
    "                            split='train',\n",
    "                            preprocess=preprocess,\n",
    "                            transform=data_transform)\n",
    "\n",
    "eeg_val = EEG_Data(data_root,\n",
    "                        split='val',\n",
    "                        preprocess=preprocess,\n",
    "                        transform=data_transform)\n",
    "eeg_test = EEG_Data(data_root,\n",
    "                        split='test',\n",
    "                        preprocess=lambda x:test_data_prep(x, 2, cutoff),\n",
    "                        transform=data_transform)\n",
    "print(\"val split: \", len(eeg_val))\n",
    "print(\"train split: \", len(eeg_train))\n",
    "print(eeg_train[0][0].shape)\n",
    "print(eeg_train[0][0][:,0:18,:].shape)\n",
    "# Create the dataloaders\n",
    "# Define the batch size and number of workers\n",
    "batch_size = 64\n",
    "num_workers=2\n",
    "# Create DataLoader for trainimport torch\n",
    "train_loader = DataLoader(eeg_train,\n",
    "                        batch_size=batch_size,\n",
    "                        num_workers=num_workers,\n",
    "                        shuffle=True)\n",
    "val_loader = DataLoader(eeg_val,\n",
    "                        batch_size=batch_size,\n",
    "                        num_workers=num_workers,\n",
    "                        shuffle=False)\n",
    "test_loader = DataLoader(eeg_test,\n",
    "                        batch_size=batch_size,\n",
    "                        num_workers=num_workers,\n",
    "                        shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MafiXWN0EdYv",
    "outputId": "5eb43e6e-1b55-4655-92ca-146b294ff33e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/60:   0%|          | 0/119 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/60:  83%|████████▎ | 99/119 [00:05<00:01, 13.39it/s, loss=1.28]"
     ]
    }
   ],
   "source": [
    "import json\n",
    "# @title Data loading\n",
    "seed_everything(0)\n",
    "\n",
    "model = EEGNet()\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Let's test_loaderss function, your implementation and the built-in loss function should\n",
    "# be almost identical.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Train the model\n",
    "train(model,\n",
    "      train_loader,\n",
    "      test_loader,\n",
    "      optimizer,\n",
    "      criterion,\n",
    "      device,\n",
    "      num_epochs=60)\n",
    "\n",
    "avg_loss, accuracy = evaluate(model, test_loader, criterion, device)\n",
    "\n",
    "print(\"avg_loss\", avg_loss)\n",
    "print(\"accuracy\", accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_loss 0.8260683247021267\n",
      "accuracy 0.6839729119638827\n"
     ]
    }
   ],
   "source": [
    "avg_loss, accuracy = evaluate(model, test_loader, criterion, device)\n",
    "\n",
    "print(\"avg_loss\", avg_loss)\n",
    "print(\"accuracy\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1, 1, 64])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import utils\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "\n",
    "# self.F1 = 8\n",
    "# self.F2 = 16\n",
    "# self.D = 2\n",
    "\n",
    "# # Conv2d(in,out,kernel,stride,padding,bias)\n",
    "# self.conv1 = nn.Sequential(\n",
    "#     nn.Conv2d(1, self.F1, (1, 64), padding=(0, 32), bias=False),\n",
    "#     nn.BatchNorm2d(self.F1)\n",
    "# )\n",
    "\n",
    "# self.conv2 = nn.Sequential(\n",
    "#     nn.Conv2d(self.F1, self.D*self.F1, (18, 1), groups=self.F1, bias=False),\n",
    "#     nn.BatchNorm2d(self.D*self.F1),\n",
    "#     nn.ELU(),\n",
    "#     nn.AvgPool2d((1, 4)),\n",
    "#     nn.Dropout(0.5)\n",
    "# )\n",
    "\n",
    "# self.Conv3 = nn.Sequential(\n",
    "#     nn.Conv2d(self.D*self.F1, self.D*self.F1, (1, 16), padding=(0, 8), groups=self.D*self.F1, bias=False),\n",
    "#     nn.Conv2d(self.D*self.F1, self.F2, (1, 1), bias=False),\n",
    "#     nn.BatchNorm2d(self.F2),\n",
    "#     nn.ELU(),\n",
    "#     nn.AvgPool2d((1, 8)),\n",
    "#     nn.Dropout(0.5)\n",
    "# )\n",
    "\n",
    "figure(figsize=(8, 6), dpi=80)\n",
    "def visTensor(tensor, ch=0, allkernels=False, nrow=1, padding=1): \n",
    "    n,c,w,h = tensor.shape\n",
    "\n",
    "    if allkernels: tensor = tensor.view(n*c, -1, w, h)\n",
    "    elif c != 3: tensor = tensor[:,ch,:,:].unsqueeze(dim=1)\n",
    "\n",
    "    rows = np.min((tensor.shape[0] // nrow + 1, 64))    \n",
    "    grid = utils.make_grid(tensor, nrow=nrow, normalize=True, padding=padding)\n",
    "    # plt.figure( figsize=(nrow,rows) )\n",
    "    plt.imshow(grid.numpy().transpose((1, 2, 0)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "filter = model.conv1[0].weight.data.clone()\n",
    "print(filter.shape)\n",
    "visTensor(filter, ch=0, allkernels=True)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qp31VVTp2pgf",
    "outputId": "da86b7db-82f0-4c37-bb9c-56c5717e81dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_loss 0.8258780837059021\n",
      "accuracy 0.6523702031602708\n"
     ]
    }
   ],
   "source": [
    "\n",
    "avg_loss, accuracy = evaluate(model, test_loader, criterion, device)\n",
    "\n",
    "print(\"avg_loss\", avg_loss)\n",
    "print(\"accuracy\", accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
