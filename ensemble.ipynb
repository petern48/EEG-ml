{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNLouVcrvehI9HLAqHiCNNM"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Ensemble Model"
      ],
      "metadata": {
        "id": "9PXssmgYFSah"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# INSERT MODEL AND DATA PREPROCESSING CODE HERE\n",
        "models = []\n",
        "input_shape = (0,)"
      ],
      "metadata": {
        "id": "5S_sPl_2FYRN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_test = len(x_test)\n",
        "# x_test, y_test\n",
        "corrects = 0\n",
        "\n",
        "# could create torch/keras batches or large tensors for each instead\n",
        "\n",
        "for i in range(num_test):\n",
        "    preds = []\n",
        "    for model in models():\n",
        "        pred = model(x_test[i])\n",
        "        # pred = np.argmax maybe (get raw predictions)\n",
        "        preds.append(pred)\n",
        "\n",
        "    preds = np.array(preds)\n",
        "\n",
        "    # Consider handling ties\n",
        "    overall_pred = np.bincount(x).argmax()\n",
        "\n",
        "    if overall_pred == y_tests[i]:\n",
        "        corrects += 1\n",
        "\n",
        "test_accuracy = corrects / num_test\n",
        "print(f\"Test Accuracy: {test_accuracy}\")"
      ],
      "metadata": {
        "id": "ZqfJqRNPFcN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqbNGS2YFRIy"
      },
      "outputs": [],
      "source": []
    }
  ]
}